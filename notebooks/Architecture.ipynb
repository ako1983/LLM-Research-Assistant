{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec00a0d-8e23-4e29-ae1f-43b67e854d46",
   "metadata": {},
   "source": [
    "# Agent Architecture Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ea4c330-d141-4415-921f-2c74a1a78db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "my_api_key = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec43fcda-cec1-452f-98f8-411127e51622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic(\n",
    "    api_key=my_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1803844-dfa8-4aa3-aa9a-7f6c2ade9a8c",
   "metadata": {},
   "source": [
    "## 1. Query Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3de08-f338-4e44-af0d-bc890c96f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent architecture\n",
    "class ResearchAgent:\n",
    "    def __init__(self, llm, retriever, tools=None):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever  # Your existing ChromaDB retriever\n",
    "        self.tools = tools or {}\n",
    "        self.query_router = QueryRouter(llm)\n",
    "    \n",
    "    def process_query(self, query):\n",
    "        # Main entry point for processing queries\n",
    "        query_type = self.query_router.classify_query(query)\n",
    "        \n",
    "        if query_type == \"direct_knowledge\":\n",
    "            return self.direct_response(query)\n",
    "        elif query_type == \"research_needed\":\n",
    "            return self.research_response(query)\n",
    "        elif query_type == \"tool_required\":\n",
    "            return self.tool_response(query)\n",
    "    \n",
    "    def direct_response(self, query):\n",
    "        # Generate response directly from LLM\n",
    "        # ...\n",
    "    \n",
    "    def research_response(self, query):\n",
    "        # Get documents from retriever\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        # Generate response with context\n",
    "        # ...\n",
    "    \n",
    "    def tool_response(self, query):\n",
    "        # Identify required tool\n",
    "        tool_name = self.query_router.identify_tool(query)\n",
    "        tool = self.tools.get(tool_name)\n",
    "        # Execute tool and generate response\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebded10e-2136-40e8-8201-0338ce2369db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "class QueryRouter:\n",
    "    def __init__(self, llm: ChatOpenAI, prompt_path: str):\n",
    "        self.llm = llm\n",
    "        self.prompt_template = PromptTemplate.from_file(\n",
    "            prompt_path,\n",
    "            input_variables=[\"KNOWLEDGE_BASE\", \"AVAILABLE_FUNCTIONS\", \"USER_QUERY\"]\n",
    "        )\n",
    "    \n",
    "    def classify_query(self, query: str, knowledge_base: str =\"\", availble_functions: str = \"\")-> str:\n",
    "        \"\"\"\n",
    "        Use the LLM to classify the query based on the provided prompt.\n",
    "        Returns one of: 'direct_knowledge', 'research_needed', or 'tool_required'.\n",
    "        \"\"\"\n",
    "        filled_prompt = self.prompt_template.format(\n",
    "            KNOWLEDGE_BASE=knowledge_base,\n",
    "            AVAILABLE_FUNCTIONS=available_functions,\n",
    "            USER_QUERY=query\n",
    "        )\n",
    "        response = self.llm([ HumanMessage(content=filled_prompt)]).content.lower()\n",
    "        if \"function call\" in response:\n",
    "            return \"tool_required\"\n",
    "        elif \"need for more context\" in response:\n",
    "            return \"research_needed\"\n",
    "        elif \"direct answer\" in response:\n",
    "            return \"direct_knowledge\"\n",
    "        else:\n",
    "            # Fallback to research if unclear\n",
    "            return \"research_needed\"\n",
    "\n",
    "        \n",
    "    # def identify_tool(self, query):\n",
    "    #     # Determine which tool is needed\n",
    "    #     # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a13b5646-02ab-4b2d-8999-89616cf4d311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11044/3461779143.py:8: DeprecationWarning: `input_variables' is deprecated and ignored.\n",
      "  self.prompt_template = PromptTemplate.from_file(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'available_functions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m router \u001b[38;5;241m=\u001b[39m QueryRouter(llm\u001b[38;5;241m=\u001b[39mllm, prompt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_classification_prompt_template.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the main differences between GPT-3.5 and GPT-4?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m classification \u001b[38;5;241m=\u001b[39m \u001b[43mrouter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery Type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, classification)\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mQueryRouter.classify_query\u001b[0;34m(self, query, knowledge_base, availble_functions)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclassify_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, knowledge_base: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, availble_functions: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    Use the LLM to classify the query based on the provided prompt.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    Returns one of: 'direct_knowledge', 'research_needed', or 'tool_required'.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     filled_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_template\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     19\u001b[0m         KNOWLEDGE_BASE\u001b[38;5;241m=\u001b[39mknowledge_base,\n\u001b[0;32m---> 20\u001b[0m         AVAILABLE_FUNCTIONS\u001b[38;5;241m=\u001b[39m\u001b[43mavailable_functions\u001b[49m,\n\u001b[1;32m     21\u001b[0m         USER_QUERY\u001b[38;5;241m=\u001b[39mquery\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm([ HumanMessage(content\u001b[38;5;241m=\u001b[39mfilled_prompt)])\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction call\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'available_functions' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "router = QueryRouter(llm=llm, prompt_path=\"query_classification_prompt_template.txt\")\n",
    "\n",
    "query = \"What are the main differences between GPT-3.5 and GPT-4?\"\n",
    "classification = router.classify_query(query)\n",
    "\n",
    "print(\"Query Type:\", classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d5ea1f-b126-49f9-a914-9f268560b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "class QueryClassifier(dspy.Signature):\n",
    "    \"\"\"Classify a user query as requiring direct knowledge, research, or tools.\"\"\"\n",
    "    query = dspy.InputField()\n",
    "    query_type = dspy.OutputField(desc=\"One of: direct_knowledge, research_needed, tool_required\")\n",
    "    reasoning = dspy.OutputField(desc=\"Explanation for the classification\")\n",
    "\n",
    "class DirectResponseGenerator(dspy.Signature):\n",
    "    \"\"\"Generate a response using only the model's knowledge.\"\"\"\n",
    "    query = dspy.InputField()\n",
    "    response = dspy.OutputField()\n",
    "\n",
    "# Additional signature classes for other components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e779f-86a4-4b3e-b0a1-42476656cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryClassifierModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.classifier = dspy.Predict(QueryClassifier)\n",
    "    \n",
    "    def forward(self, query):\n",
    "        return self.classifier(query=query)\n",
    "\n",
    "# Additional modules for other components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37567c01-4669-4e3e-8970-3348caffd15b",
   "metadata": {},
   "source": [
    "## 2. Direct Knowledge Responder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433486f9-85ef-4b6b-879f-8a6d22258676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd694829-e52b-4c10-9b78-320e593cbc83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f223e2c-3652-48ab-a020-44b09f912a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1deb848-38ca-4ae3-bb57-993cec0320a5",
   "metadata": {},
   "source": [
    "## 3. RAG-Enhanced Responder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25514404-96e9-4f09-b7f2-dfe8a17c4f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d37ed4f-1363-45f2-992c-b6194f95a6a9",
   "metadata": {},
   "source": [
    "## 4. Tool Caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ae615-9560-4914-a2be-ba135908fcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1340a-5ce8-490b-a311-e00b49504edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28d024b0-f0f9-4076-89a7-b0f2c8bc38f6",
   "metadata": {},
   "source": [
    "## 5. Multi-Step Reasoning Coordinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d6021-1d24-4ef2-a93d-6c8703d94ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MindsDB_venv)",
   "language": "python",
   "name": "mindsdb_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
